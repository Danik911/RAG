{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from naive_rag.helpers.utils import retrieve_documents\n",
    "# %%capture\n",
    "!pip install llama-index==0.10.37 openai==1.30.1 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ],
   "id": "e84039d912970123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables (fixing potential missing file argument issue)\n",
    "load_dotenv()\n",
    "current_dir = Path.cwd()       # or Path().resolve()\n",
    "helpers_path = current_dir.parent / 'helpers'\n",
    "sys.path.append(str(helpers_path))\n"
   ],
   "id": "f47685adc4b170c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(r\"C:\\Users\\anteb\\PycharmProjects\\JupyterProject\\.env\")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Debug: Print environment variables to verify they are loaded\n",
    "print(\"CO_API_KEY:\", os.environ.get('CO_API_KEY'))\n",
    "print(\"OPENAI_API_KEY:\", os.environ.get('OPENAI_API_KEY'))\n",
    "print(\"QDRANT_URL:\", os.environ.get('QDRANT_URL'))\n",
    "print(\"QDRANT_API_KEY:\", os.environ.get('QDRANT_API_KEY'))\n",
    "\n",
    "CO_API_KEY = os.environ.get('CO_API_KEY') or getpass(\"Enter CO_API_KEY: \")\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass(\"Enter OPENAI_API_KEY: \")\n",
    "QDRANT_URL = os.environ.get('QDRANT_URL') or getpass(\"Enter QDRANT_URL: \")\n",
    "QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY') or getpass(\"Enter QDRANT_API_KEY: \")\n"
   ],
   "id": "d8478b83b2ff79ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = Cohere(model=\"command-r-plus\", api_key=CO_API_KEY)\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")"
   ],
   "id": "b22a874c2c0a5b10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:48:57.245226Z",
     "start_time": "2025-03-10T15:48:52.796847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[r\"C:\\Users\\anteb\\PycharmProjects\\JupyterProject\\naive_rag\\data\\raw\\2502.20364.pdf\"],\n",
    "    filename_as_id=True).load_data()"
   ],
   "id": "db9776e5f0016ed8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:49:54.747735Z",
     "start_time": "2025-03-10T15:49:41.679553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from naive_rag.helpers.IngestionCacheManager import SmartIngestionCache\n",
    "from qdrant_client import QdrantClient\n",
    "from datetime import datetime\n",
    "from llama_index.core.ingestion import IngestionCache\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionCache, IngestionPipeline\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"rag_articles\")\n",
    "\n",
    "ingest_cache = SmartIngestionCache().get_cache()\n",
    "\n",
    "# create pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        TokenTextSplitter(chunk_size=256, chunk_overlap=16),\n",
    "        Settings.embed_model\n",
    "    ],\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    vector_store=vector_store,\n",
    "    cache=ingest_cache,\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents = documents)"
   ],
   "id": "9aafcea1d2fb4360",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:50:01.370704Z",
     "start_time": "2025-03-10T15:50:01.358172Z"
    }
   },
   "cell_type": "code",
   "source": "nodes[0].__dict__.keys()",
   "id": "f792190304146535",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id_', 'embedding', 'metadata', 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys', 'relationships', 'text', 'mimetype', 'start_char_idx', 'end_char_idx', 'text_template', 'metadata_template', 'metadata_seperator'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:50:50.692394Z",
     "start_time": "2025-03-10T15:50:50.675296Z"
    }
   },
   "cell_type": "code",
   "source": "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)",
   "id": "957dae0d04690d06",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:50:53.586128Z",
     "start_time": "2025-03-10T15:50:53.575016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=7,\n",
    "    return_sources=True\n",
    "    )"
   ],
   "id": "95e33c5ab54d1753",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:51:00.057767Z",
     "start_time": "2025-03-10T15:50:56.347742Z"
    }
   },
   "cell_type": "code",
   "source": "retrieved_nodes = retriever.retrieve(\"How RAG can improve the user experience?\")",
   "id": "142c3e7aa3327a39",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:51:02.088698Z",
     "start_time": "2025-03-10T15:51:02.076366Z"
    }
   },
   "cell_type": "code",
   "source": "len(retrieved_nodes)",
   "id": "e0b4774c47bb3e0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:51:04.636633Z",
     "start_time": "2025-03-10T15:51:04.628602Z"
    }
   },
   "cell_type": "code",
   "source": "print(retrieved_nodes[0].get_text())",
   "id": "782c14bc2b9be79d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TELERAG: Efficient Retrieval-Augmented Generation Inference\n",
      "with Lookahead Retrieval\n",
      "Chien-Yu Lin1,∗Keisuke Kamahori1,∗Yiyu Liu2,†Xiaoxiang Shi2,†Madhav Kashyap1Yile Gu1\n",
      "Rulin Shao1Zihao Ye1Kan Zhu1Stephanie Wang1Arvind Krishnamurthy1Rohan Kadekodi1\n",
      "Luis Ceze1Baris Kasikci1\n",
      "1University of Washington2Shanghai Jiao Tong University\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) extends large lan-\n",
      "guage models (LLMs) with external data sources to en-\n",
      "hance factual correctness and domain coverage. Modern RAG\n",
      "pipelines rely on large datastores, leading to system chal-\n",
      "lenges in latency-sensitive deployments, especially when lim-\n",
      "ited GPU memory is available. To address these challenges,\n",
      "we propose TELERAG , an efficient inference system that re-\n",
      "duces RAG latency with minimal GPU memory requirements.\n",
      "The core innovation of TELERAG islookahead retrieval ,\n",
      "a prefetching mechanism that anticipates required data and\n",
      "transfers it from CPU to GPU in parallel with LLM gener-\n",
      "ation. By leveraging the modularity of RAG pipelines, the\n",
      "inverted file index (IVF) search algorithm and similarities be-\n",
      "tween queries, TELERAG optimally overlaps data movement\n",
      "and computation. Experimental results show that TELERAG\n",
      "reduces end-to-end RAG inference latency by up to 1.72 ×\n",
      "on average compared to state-of-the-art systems, enabling\n",
      "faster, more memory-efficient deployments of advanced RAG\n",
      "applications.\n",
      "1 Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a pow-\n",
      "erful technique to enhance large language models (LLMs) by\n",
      "integrating them with external databases [13, 29, 57]. Dur-\n",
      "ing inference, RAG retrieves relevant content from external\n",
      "data sources, usually indexed as vector datastores, to miti-\n",
      "gate issues such as hallucinations [50, 63, 74] and incorporate\n",
      "up-to-date or private information [36, 64].\n",
      "Modern RAG applications share two key characteristics.\n",
      "(1) RAG applications are built as modular pipelines as shown\n",
      "in Figure 1a, where a single query undergoes multiple rounds\n",
      "of LLM calls and retrievals, each of which serves different\n",
      "∗Equal contribution.†Work done during internship at UW. Correspon-\n",
      "dence to: Chien-Yu Lin <cyulin@cs.washington.edu>.\n",
      "Pre-retrieval\n",
      "generationRetrievalPost-retrieval\n",
      "generation(a) Typical pipeline stages of a RAG application.\n",
      "Pre-retrieval\n",
      "generationGPUCPU\n",
      "GPU\n",
      "RetrievalPost-retrieval\n",
      "generationIdeal Scenario (r equir es substantial GPU memory)\n",
      "Pre-retrieval\n",
      "generationGPUCPU CPU Retrieval\n",
      "Post-retrieval\n",
      "generationTypical CPU-offload\n",
      "Pre-retrieval\n",
      "generationGPUCPUCPU\n",
      "Retrieval\n",
      "Post-retrieval\n",
      "generationTeleRAG: Lookahead Retrieval\n",
      "Prefetch\n",
      "CPU → GPULatency:\n",
      "Cost:Low\n",
      "High\n",
      "Latency:\n",
      "Cost:Latency:\n",
      "Cost:\n",
      "High\n",
      "Low\n",
      "Low\n",
      "Low\n",
      "GPU\n",
      "Retrieval\n",
      "(b) Different scenarios for RAG and the illustration of the proposed\n",
      "lookahead retrieval mechanism. It prefetches relevant data for re-\n",
      "trieval from CPU to GPU, overlaps data transfer with the pre-retrieval\n",
      "stage, and accelerates retrieval with GPU-CPU cooperation.\n",
      "Figure 1: (a) Illustrations of RAG pipeline stages. (b)\n",
      "Overview of TELERAG and comparison to baseline systems.\n",
      "functions to improve overall output quality. For example,\n",
      "query transformation rounds [10, 27, 71, 100] generally oc-\n",
      "cur before retrieval to refine the user’s query with LLMs\n",
      "(pre-retrieval generation). (2) RAG’s datastores are typically\n",
      "large , supported by recent works demonstrating that increas-\n",
      "ing the size of the datastore positively affects the performance\n",
      "of RAG applications [15, 33, 50, 64, 77].\n",
      "These characteristics create significant challenges for ef-\n",
      "ficient RAG inference, especially in latency-sensitive appli-\n",
      "cations such as customer chatbots [7, 18, 83], financial analy-\n",
      "sis [62, 65], and emergency medical diagnosis [31, 53]. First,\n",
      "the latency of RAG systems is inherently increased due to\n",
      "1arXiv:2502.20969v1  [cs.DC]  28 Feb 2025\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:51:09.349472Z",
     "start_time": "2025-03-10T15:51:09.341748Z"
    }
   },
   "cell_type": "code",
   "source": "print(retrieved_nodes[3].get_score())",
   "id": "590cabfb8fc273b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5403015\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:51:12.985830Z",
     "start_time": "2025-03-10T15:51:12.718420Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.persist('./pipeline_storage')",
   "id": "990b04280d1a3cc6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dcfde74227bbe437",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
